{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc4c0de2-b9ee-473d-9061-b0755d1c4e27",
   "metadata": {
    "id": "bc4c0de2-b9ee-473d-9061-b0755d1c4e27"
   },
   "source": [
    "Import necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "119c744a-26aa-4388-aeb4-d2fc4e70bfa5",
   "metadata": {
    "id": "119c744a-26aa-4388-aeb4-d2fc4e70bfa5"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn as sl\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import email\n",
    "from collections import Counter as count\n",
    "import re\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a788aa-a72d-472e-9f5e-6b96b7506d8f",
   "metadata": {},
   "source": [
    "Google colab dependency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aKplKvsG9fEh",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aKplKvsG9fEh",
    "outputId": "5fe27e3e-1620-4b55-821e-be03823200ed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b56fb5-5fe6-492b-9921-92dcc3d8978a",
   "metadata": {
    "id": "61b56fb5-5fe6-492b-9921-92dcc3d8978a"
   },
   "source": [
    "Import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a785762-41d4-4f8e-b067-8692a2cd6386",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "0a785762-41d4-4f8e-b067-8692a2cd6386",
    "outputId": "82cd2718-bc02-4b36-c487-1dd1a9313b0c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>file</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>data/000/000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>data/000/001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>data/000/002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>data/000/003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>data/000/004</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label          file\n",
       "0      0  data/000/000\n",
       "1      1  data/000/001\n",
       "2      1  data/000/002\n",
       "3      0  data/000/003\n",
       "4      1  data/000/004"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"labels\",'r') as f:\n",
    "    # Add files with label to\n",
    "    files = pd.DataFrame([line.strip().replace(\"../\",\"\").split(' ') for line in f.readlines()], columns=['label','file'])\n",
    "    files[\"label\"] = np.where(files[\"label\"] == \"ham\", 0, 1)\n",
    "\n",
    "with open(\"stop_words.txt\", 'r') as f:\n",
    "    stop_words = [line.strip() for line in f.readlines()]\n",
    "# stop_words\n",
    "files.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d64fea-d0c5-496f-814f-fc67474dba6b",
   "metadata": {},
   "source": [
    "Create function to clean the email message. Remove all non alphabet characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "237230ed-10b0-4a37-a3a7-19f037ebd8d3",
   "metadata": {
    "id": "237230ed-10b0-4a37-a3a7-19f037ebd8d3"
   },
   "outputs": [],
   "source": [
    "CLEANR = [\"a\",\"b\",\"c\",\"d\",\"e\",\"f\",\"g\",\"h\",\"i\",\"j\",\"k\",\"l\",\"m\",\"n\",\"o\",\"p\",\"q\",\"r\",\"s\",\"t\",\"u\",\"v\",\"w\",\"x\",\"y\",\"z\"]\n",
    "LINKCLEANR = re.compile(r'https?://\\S+')\n",
    "HTMLCLEANR = re.compile(r'<[^>]*>|&([a-z0-9]+);')\n",
    "\n",
    "def clean_email(mail):\n",
    "    if len(mail) == 0:\n",
    "        return \"\"\n",
    "    # Ignore case\n",
    "    mail = mail.lower()\n",
    "    # Remove tags\n",
    "    mail = re.sub(HTMLCLEANR,\"\",mail)\n",
    "    # Split to words\n",
    "    words = mail.split()\n",
    "\n",
    "    for idx, word in enumerate(words):\n",
    "        # Skip cleaning if all characters are in alphabet\n",
    "        # if any(True for letter in word if letter in CLEANR else False):\n",
    "        if not word.isalpha():\n",
    "            # Remove links\n",
    "            word = re.sub(LINKCLEANR,\"\",word)\n",
    "            # Remove special characters and numbers\n",
    "            word = ''.join(letter for letter in word if letter in CLEANR)\n",
    "\n",
    "        # Replace stop words with blank\n",
    "        if word in stop_words:\n",
    "            words[idx] = ''\n",
    "        else:\n",
    "            words[idx] = word\n",
    "    # Rejoin message\n",
    "    cleaned_message = ' '.join([word for word in words if word != ''])\n",
    "    return cleaned_message"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0866ff64-df04-4d63-bee6-a31943c5c395",
   "metadata": {},
   "source": [
    "On some files, replies are appended after the body separated by a timestamp. This function splits the body using the timestamps and reads it as an email to remove the headers. All email messages are then appended into a single message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da3f45bd-2f24-4e7c-a44a-58caf5cd2dab",
   "metadata": {
    "id": "da3f45bd-2f24-4e7c-a44a-58caf5cd2dab"
   },
   "outputs": [],
   "source": [
    "TIMESTAMP = re.compile(r'\\d{1,2}-[a-z]{3}-\\d{4}.*\\b')\n",
    "\n",
    "def split_replies(file):\n",
    "    # Skip function if the email does not contain replies\n",
    "    if re.findall(TIMESTAMP, file.lower()) == []:\n",
    "        return file\n",
    "\n",
    "    body = ''\n",
    "    # Split by timestamp\n",
    "    reply = re.split(TIMESTAMP, file.lower())\n",
    "    for string in reply:\n",
    "        # Read reply\n",
    "        raw = email.message_from_string(string.lstrip())\n",
    "        # Add reply to the body\n",
    "        body += read_email(raw) + ' '\n",
    "    # Return all replies\n",
    "    return body"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20426af5-fcb6-4fa1-a77c-514e4b9d3819",
   "metadata": {},
   "source": [
    "Reads the email.message data provided. Only decodes text and decodes non utf-8 format into the ISO 8859-1 format.\n",
    "For multipart emails, it reads all parts and ignores non-text messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf19d217-5e06-4230-ba65-767d720e765c",
   "metadata": {
    "id": "bf19d217-5e06-4230-ba65-767d720e765c"
   },
   "outputs": [],
   "source": [
    "def read_email(raw):\n",
    "    body = \"\"\n",
    "    # If email is not multipart\n",
    "    if not raw.is_multipart():\n",
    "        # Only extract text\n",
    "        if raw.get_content_maintype() == 'text':\n",
    "            body = raw.get_payload(None,True)\n",
    "            try:\n",
    "                # Try decoding if format results in bytes array\n",
    "                body = body.decode('ISO-8859-1')\n",
    "            except AttributeError:\n",
    "                pass\n",
    "        return body\n",
    "    # If email is multipart, read all parts\n",
    "    for part in raw.walk():\n",
    "        # Ignore non text parts\n",
    "        if part == None or part.get_content_maintype() != 'text':\n",
    "            continue\n",
    "\n",
    "        message = part.get_payload(None, True)\n",
    "        # Skip blank messages\n",
    "        if message == None or message == '':\n",
    "            continue\n",
    "        try:\n",
    "            # Try decoding if format results in bytes array\n",
    "            message = message.decode('ISO-8859-1')\n",
    "        finally:\n",
    "            body += message + ' '\n",
    "    return body"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa79f12-18f2-4de1-b99d-e5b0093950ee",
   "metadata": {},
   "source": [
    "Reads the email file and passes the resulting body to the be split if possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe1d5a6-a24f-4eb8-adbc-ea8526e3753e",
   "metadata": {
    "id": "afe1d5a6-a24f-4eb8-adbc-ea8526e3753e"
   },
   "outputs": [],
   "source": [
    "def read_email_file(file):\n",
    "    body = \"\"\n",
    "    try:\n",
    "        # Get email.message and pass to read_email\n",
    "        raw = email.message_from_file(file)\n",
    "        body = read_email(raw)\n",
    "        # Split replies\n",
    "        body = split_replies(body)\n",
    "\n",
    "    except UnicodeDecodeError:\n",
    "        print(\"bruh\")\n",
    "    return body"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a3ea3c4-0981-4349-8ade-37c0f6529ce7",
   "metadata": {},
   "source": [
    "Reads all files in the dataframe and cleans them. The cleaned messages are then saved to csv to avoid rerunning the cleaning process during testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95eeb917-2668-46f4-ae6e-f017e0c38f78",
   "metadata": {
    "id": "95eeb917-2668-46f4-ae6e-f017e0c38f78"
   },
   "outputs": [],
   "source": [
    "def clean_and_tokenize(files):\n",
    "    contents = []\n",
    "    for loc in files['file']:\n",
    "        print(\"Cleaning Email - \", loc)\n",
    "        # Read email file\n",
    "        with open(f'{loc}', 'r', encoding = 'ISO-8859-1') as f:\n",
    "            read = read_email_file(f)\n",
    "            try:\n",
    "                # Final pass\n",
    "                read = read.decode('ISO-8859-1')\n",
    "            except:\n",
    "                pass\n",
    "            finally:\n",
    "                # Clean email\n",
    "                content = clean_email(read)\n",
    "            contents.append(content)\n",
    "    files['email_content'] = contents\n",
    "    # Save to csv to avoid rerunning\n",
    "    files.to_csv('data.csv', encoding='utf-8')\n",
    "    print('Cleaning data complete.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "343927c6-262b-4ead-a82f-44acc072eb93",
   "metadata": {},
   "source": [
    "Builds the vocabulary. Cleaned messages are split and all instances of each word are counted before getting the most common ones as part of the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "43c86646-0f4c-4f4c-ba14-ea008864e2ea",
   "metadata": {
    "id": "43c86646-0f4c-4f4c-ba14-ea008864e2ea"
   },
   "outputs": [],
   "source": [
    "def build_vocabulary(emails, vocabulary=1000):\n",
    "    all_words = []\n",
    "    for content in emails:\n",
    "        all_words.extend(str(content).split())\n",
    "    counts = count(all_words)\n",
    "    vocab = counts.most_common(vocabulary)\n",
    "    # print(vocab)\n",
    "    return [pair[0] for pair in vocab]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "393a23e9-4f4a-4b4c-ba46-52b53e82da6a",
   "metadata": {},
   "source": [
    "Generates a feature matrix. A 1 in any cell means that message(row) contains the word(column) and 0 otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f51ab507-4d9a-4ed4-b070-441e882cd676",
   "metadata": {
    "id": "f51ab507-4d9a-4ed4-b070-441e882cd676"
   },
   "outputs": [],
   "source": [
    "def generate_feature_matrix(emails, vocabulary):\n",
    "    feature_matrix = np.zeros((len(emails), len(vocabulary)))\n",
    "    for i, content in enumerate(emails):\n",
    "        words = str(content).split()\n",
    "        # print(words)\n",
    "        for word in words:\n",
    "            if word in vocabulary:\n",
    "                feature_matrix[i, vocabulary.index(word)] = 1\n",
    "    # print(feature_matrix)\n",
    "    return feature_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8751a56-8b69-4e27-b078-cbc92f467db2",
   "metadata": {},
   "source": [
    "Computes priors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5b0ae818-e945-42a5-bbb5-3abaf170c13c",
   "metadata": {
    "id": "5b0ae818-e945-42a5-bbb5-3abaf170c13c"
   },
   "outputs": [],
   "source": [
    "def compute_priors(label):\n",
    "    spam = sum(label)\n",
    "    return sum(label) / len(label), len(label)-spam / len(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80482851-2c97-4fdb-b329-e93e998d71c8",
   "metadata": {},
   "source": [
    "Computes the chances of the word being spam or ham with smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "796c7205-4695-4934-8c2f-736855a6bcda",
   "metadata": {
    "id": "796c7205-4695-4934-8c2f-736855a6bcda"
   },
   "outputs": [],
   "source": [
    "def compute_likelihoods(x, y, vocabulary, smoothing=1.0):\n",
    "    word_count= len(vocabulary)\n",
    "\n",
    "    # Count occurrences for spam and ham\n",
    "    spam = np.zeros(word_count)\n",
    "    ham = np.zeros(word_count)\n",
    "\n",
    "    for i in range(len(y)):\n",
    "        if y[i] == 1:\n",
    "            spam += x[i]\n",
    "        else:\n",
    "            ham += x[i]\n",
    "\n",
    "    # Total counts of words in spam and ham\n",
    "    total_spam = np.sum(spam)\n",
    "    total_ham = np.sum(ham)\n",
    "\n",
    "    # Compute likelihoods with Laplace smoothing\n",
    "    spam_chance = (spam + smoothing) / (total_spam + smoothing * word_count)\n",
    "    ham_chance = (ham + smoothing) / (total_ham + smoothing * word_count)\n",
    "\n",
    "    return spam_chance, ham_chance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "568d30ac-88f8-44fe-ac2a-6ab8222e39d7",
   "metadata": {},
   "source": [
    "Computes the log probabilities of the email being spam or ham"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f62622d3-418f-45f5-bd7b-828cb7824a53",
   "metadata": {
    "id": "f62622d3-418f-45f5-bd7b-828cb7824a53"
   },
   "outputs": [],
   "source": [
    "def classify_email(email, spam_chance, ham_chance, spam_priori, ham_priori, underflow = 1e-10):\n",
    "    log_spam = np.log(spam_priori) + np.sum(email * np.log(spam_chance))\n",
    "    log_ham = np.log(ham_priori + underflow) + np.sum(email * np.log(ham_chance + underflow))\n",
    "\n",
    "    # Classify as spam if log_prob_spam is greater, otherwise ham\n",
    "    return 1 if log_spam > log_ham else 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10b0df69-7aa3-44ea-b5a4-fe607c52693a",
   "metadata": {},
   "source": [
    "Classifies test emails based on the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "82ea7a31-ca78-4732-b133-f808e8d119e6",
   "metadata": {
    "id": "82ea7a31-ca78-4732-b133-f808e8d119e6"
   },
   "outputs": [],
   "source": [
    "def classify_test(x, spam_chance, ham_chance, spam_priori, ham_priori):\n",
    "    classes = []\n",
    "    for email in x:\n",
    "        email_class = classify_email(email, spam_chance, ham_chance, spam_priori, ham_priori)\n",
    "        classes.append(email_class)\n",
    "    return classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5acf2e3-b0e3-4919-858f-48bc3e97e754",
   "metadata": {},
   "source": [
    "Implement Naive Bayes Spam Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5b1fa769-9685-411a-a6fd-4f5012c42367",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5b1fa769-9685-411a-a6fd-4f5012c42367",
    "outputId": "3e0e5694-dad7-4f06-9614-a9ddadf7e211"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6042246701367873\n",
      "Precision: 0.9914456800684346\n",
      "Recall: 0.4163448585541087\n"
     ]
    }
   ],
   "source": [
    "def naive_bayes_spam_filter():\n",
    "    # Clean data\n",
    "    # clean_and_tokenize(files)\n",
    "    \n",
    "    # Load cleaned dataset\n",
    "    data = pd.read_csv(\"data.csv\",index_col=0)\n",
    "\n",
    "    # Split label and email contents\n",
    "    label = data.loc[:, ['label']]\n",
    "    content = data.loc[:, ['email_content']]\n",
    "\n",
    "    # Split training data\n",
    "    train_label = label.iloc[:21300,:].to_numpy().flatten()\n",
    "    test_label = label.iloc[21300:,:].to_numpy().flatten()\n",
    "    train_content = content.iloc[:21300,:].to_numpy().flatten()\n",
    "    test_content = content.iloc[21300:,:].to_numpy().flatten()\n",
    "\n",
    "    # Build vocabulary from the cleaned training emails\n",
    "    vocab = build_vocabulary(train_content)\n",
    "\n",
    "    # Create feature matrices\n",
    "    train_data = generate_feature_matrix(train_content, vocab)\n",
    "    test_data = generate_feature_matrix(test_content, vocab)\n",
    "\n",
    "    # Compute priors\n",
    "    spam_priori, ham_priori = compute_priors(train_label)\n",
    "\n",
    "    # Compute likelihoods\n",
    "    spam_chance, ham_chance = compute_likelihoods(train_data, train_label, vocab)\n",
    "\n",
    "    # Classify the test set\n",
    "    y = classify_test(test_data, spam_chance, ham_chance, spam_priori, ham_priori)\n",
    "\n",
    "    # Evaluate performance\n",
    "    accuracy = accuracy_score(test_label, y)\n",
    "    precision = precision_score(test_label, y)\n",
    "    recall = recall_score(test_label, y)\n",
    "\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "    print(f\"Precision: {precision}\")\n",
    "    print(f\"Recall: {recall}\")\n",
    "\n",
    "naive_bayes_spam_filter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4561f97-4278-4073-8a20-cad5dd1b223f",
   "metadata": {
    "id": "a4561f97-4278-4073-8a20-cad5dd1b223f"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
